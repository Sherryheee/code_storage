from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, concatenate
from tensorflow.keras.models import Model

def build_model(input_dim):
    inputs = Input(shape=(input_dim,))

    # 自注意力机制
    attention = MultiHeadAttention(num_heads=8, key_dim=16)
    attention_output = attention(inputs, inputs)
    attention_output = LayerNormalization()(attention_output)
    attention_output = Dropout(0.1)(attention_output)

    # 拼接原始输入和注意力输出
    concatenated = concatenate([inputs, attention_output], axis=-1)

    # 更深的全连接层网络
    hidden_layer_1 = Dense(128, activation='relu')(concatenated)
    hidden_layer_1 = Dropout(0.3)(hidden_layer_1)
    hidden_layer_2 = Dense(64, activation='relu')(hidden_layer_1)
    hidden_layer_2 = Dropout(0.3)(hidden_layer_2)

    # 输出层
    output = Dense(1, activation='sigmoid')(hidden_layer_2)  # 二元分类问题

    # 构建模型
    model = Model(inputs=inputs, outputs=output)
    return model

# 用法示例
input_dim = 30  # 假设有30个特征
model = build_model(input_dim)
model.summary()
